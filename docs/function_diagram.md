# 主要函数示意图

## 目录
- [1. 文档加载流程](#1-文档加载流程)
- [2. 向量存储流程](#2-向量存储流程)
- [3. 问答流程](#3-问答流程)
- [4. 混合检索流程](#4-混合检索流程)
- [5. 习题生成流程](#5-习题生成流程)
- [6. 完整系统架构](#6-完整系统架构)

---

## 1. 文档加载流程

### 流程图

```
┌─────────────────────────────────────────────────────────────┐
│                    DocumentLoader                            │
│                                                              │
│  load_all_documents()                                        │
│         │                                                    │
│         ├──> os.walk(data_dir) 遍历文件                     │
│         │                                                    │
│         └──> for each file:                                 │
│                  │                                           │
│                  └──> load_document(file_path)              │
│                           │                                  │
│                           ├──> .pdf  → load_pdf()          │
│                           │      └─> PdfReader读取          │
│                           │           └─> 按页提取文本       │
│                           │                                  │
│                           ├──> .pptx → load_pptx()         │
│                           │      └─> Presentation读取       │
│                           │           └─> 按幻灯片提取文本   │
│                           │                                  │
│                           ├──> .docx → load_docx()         │
│                           │      └─> docx2txt读取           │
│                           │                                  │
│                           ├──> .txt  → load_txt()          │
│                           │      └─> open()读取             │
│                           │                                  │
│                           └──> .jpg/.png → load_image()    │
│                                 └─> EasyOCR提取文字         │
│                                                              │
│  返回：List[Dict]                                            │
│  [                                                           │
│    {                                                         │
│      "content": "文档内容",                                  │
│      "filename": "文件名",                                   │
│      "filepath": "文件路径",                                 │
│      "filetype": ".pdf/.pptx/...",                          │
│      "page_number": 页码/0                                   │
│    },                                                        │
│    ...                                                       │
│  ]                                                           │
└─────────────────────────────────────────────────────────────┘
```

### 关键函数说明

#### `DocumentLoader.load_all_documents()`
```python
功能：加载数据目录下的所有支持格式的文档
参数：无（使用config.py中的DATA_DIR）
返回：List[Dict] - 文档列表，每个文档包含内容和元数据
流程：
  1. 检查数据目录是否存在
  2. 遍历目录下所有文件
  3. 根据文件扩展名调用对应的加载方法
  4. 返回所有文档的列表
```

#### `DocumentLoader.load_pdf(file_path)`
```python
功能：加载PDF文件，按页提取内容
参数：file_path - PDF文件路径
返回：List[Dict] - 页面列表，每页包含{"text": "格式化的页面内容"}
处理：
  1. 使用PdfReader打开文件
  2. 遍历每一页
  3. 提取文本内容
  4. 格式化为"--- 第 X 页 ---\n内容\n"
```

---

## 2. 向量存储流程

### 流程图

```
┌─────────────────────────────────────────────────────────────┐
│                     VectorStore                              │
│                                                              │
│  add_documents(chunks)                                       │
│         │                                                    │
│         ├──> 准备数据列表                                    │
│         │     ├─> texts: []      (文本内容)                 │
│         │     ├─> metadatas: []  (元数据)                   │
│         │     └─> ids: []        (文档ID)                   │
│         │                                                    │
│         ├──> for chunk in chunks:                           │
│         │     ├─> 提取content                               │
│         │     ├─> 构建metadata字典                          │
│         │     │    ├─> filename                             │
│         │     │    ├─> filepath                             │
│         │     │    ├─> filetype                             │
│         │     │    ├─> page_number                          │
│         │     │    └─> chunk_id                             │
│         │     └─> 生成唯一ID (doc_0, doc_1, ...)           │
│         │                                                    │
│         ├──> 生成Embeddings                                  │
│         │     └─> for text in texts:                        │
│         │          └─> embedding = get_embedding(text)      │
│         │               └─> OpenAI API调用                  │
│         │                    └─> text-embedding-v3模型      │
│         │                                                    │
│         └──> 添加到ChromaDB                                  │
│              └─> collection.add(                            │
│                    embeddings=embeddings,                   │
│                    documents=texts,                         │
│                    metadatas=metadatas,                     │
│                    ids=ids                                  │
│                  )                                           │
│                                                              │
│  search(query, top_k)                                        │
│         │                                                    │
│         ├──> query_embedding = get_embedding(query)         │
│         │         └─> OpenAI API获取查询向量                │
│         │                                                    │
│         ├──> results = collection.query(                    │
│         │      query_embeddings=[query_embedding],          │
│         │      n_results=top_k                              │
│         │    )                                               │
│         │                                                    │
│         └──> 格式化返回结果                                  │
│              └─> [                                           │
│                    {                                         │
│                      "content": 文档内容,                    │
│                      "metadata": 元数据,                     │
│                      "distance": 相似度距离                  │
│                    },                                        │
│                    ...                                       │
│                  ]                                           │
└─────────────────────────────────────────────────────────────┘
```

### 关键函数说明

#### `VectorStore.get_embedding(text)`
```python
功能：获取文本的向量表示
参数：text - 要向量化的文本
返回：List[float] - 向量（1536维或其他维度，取决于模型）
处理：
  1. 调用OpenAI embeddings API
  2. 使用text-embedding-v3模型
  3. 返回embedding向量
```

#### `VectorStore.add_documents(chunks)`
```python
功能：批量添加文档到向量数据库
参数：chunks - 文档块列表（包含content和metadata）
返回：无
处理：
  1. 遍历所有文档块
  2. 为每个文档块生成embedding
  3. 批量添加到ChromaDB
  4. 显示进度条（使用tqdm）
```

#### `VectorStore.search(query, top_k)`
```python
功能：向量相似度搜索
参数：
  - query: 查询文本
  - top_k: 返回结果数量
返回：List[Dict] - 匹配的文档列表（按相似度排序）
处理：
  1. 将查询文本向量化
  2. 在ChromaDB中进行向量相似度搜索
  3. 返回最相似的top_k个文档
```

---

## 3. 问答流程

### 流程图

```
┌────────────────────────────────────────────────────────────────┐
│                          RAGAgent                               │
│                                                                 │
│  answer_question(query, chat_history, top_k)                   │
│         │                                                       │
│         ├──> retrieve_context(query, top_k)                    │
│         │         │                                             │
│         │         ├──> 判断是否使用混合检索                     │
│         │         │                                             │
│         │         ├──> if use_hybrid_retrieval:                │
│         │         │     └─> hybrid_retriever.hybrid_search()   │
│         │         │          └─> (见混合检索流程图)             │
│         │         │                                             │
│         │         └──> else:                                    │
│         │               └─> vector_store.search()              │
│         │                    └─> ChromaDB向量搜索              │
│         │                                                       │
│         │         返回：(context, retrieved_docs)               │
│         │                context = 格式化的上下文字符串         │
│         │                [来源1]: 文件名 第X页                  │
│         │                内容...                                │
│         │                                                       │
│         └──> generate_response(query, context, chat_history)   │
│                   │                                             │
│                   ├──> 构建消息列表                             │
│                   │     ├─> System: 助教角色提示词              │
│                   │     ├─> History: 对话历史（如果有）         │
│                   │     └─> User: 当前问题+检索到的材料         │
│                   │                                             │
│                   ├──> OpenAI API调用                          │
│                   │     └─> client.chat.completions.create(    │
│                   │           model=MODEL_NAME,                │
│                   │           messages=messages,               │
│                   │           temperature=0.7                  │
│                   │         )                                  │
│                   │                                             │
│                   └──> 返回生成的回答                           │
│                                                                 │
│  返回：生成的回答文本                                           │
└────────────────────────────────────────────────────────────────┘
```

### 详细步骤

#### 1. 检索阶段 (Retrieval)

```
用户问题：query
    ↓
retrieve_context(query, top_k=5)
    ↓
┌─────────────────────────────┐
│  检索相关文档（向量或混合）  │
└─────────────────────────────┘
    ↓
retrieved_docs = [
  {
    "content": "文档内容1",
    "metadata": {
      "filename": "课件1.pdf",
      "page_number": 3
    }
  },
  ...
]
    ↓
格式化为context字符串：
"[来源 1]: 课件1.pdf 第 3 页
内容...

[来源 2]: 讲义2.pptx 第 5 页
内容..."
```

#### 2. 生成阶段 (Generation)

```
构建Prompt：
┌──────────────────────────────────┐
│ System Prompt (系统角色)          │
│ "You are a teaching assistant..." │
└──────────────────────────────────┘
    ↓
┌──────────────────────────────────┐
│ Chat History (对话历史，可选)     │
│ User: "..."                      │
│ Assistant: "..."                 │
└──────────────────────────────────┘
    ↓
┌──────────────────────────────────┐
│ User Prompt (当前问题)            │
│ "Based on the following course   │
│ materials, please answer...      │
│                                  │
│ Course Materials:                │
│ [检索到的context]                │
│                                  │
│ Student Question: [query]"       │
└──────────────────────────────────┘
    ↓
LLM (Qwen3-max)
    ↓
生成回答
```

### 关键函数说明

#### `RAGAgent.answer_question(query, chat_history, top_k)`
```python
功能：完整的问答流程
参数：
  - query: 用户问题
  - chat_history: 对话历史（可选）
  - top_k: 检索文档数量
返回：生成的回答
流程：
  1. 检索相关上下文
  2. 生成回答
  3. 返回答案
```

#### `RAGAgent.retrieve_context(query, top_k)`
```python
功能：检索相关上下文
参数：
  - query: 查询文本
  - top_k: 检索数量
返回：(context字符串, 原始文档列表)
流程：
  1. 判断是否使用混合检索
  2. 调用对应的检索方法
  3. 格式化检索结果（添加来源信息）
  4. 返回格式化的上下文
```

#### `RAGAgent.generate_response(query, context, chat_history)`
```python
功能：基于上下文生成回答
参数：
  - query: 用户问题
  - context: 检索到的上下文
  - chat_history: 对话历史
返回：生成的回答
流程：
  1. 构建消息列表（system + history + user）
  2. 调用LLM API生成回答
  3. 返回回答内容
```

---

## 4. 混合检索流程

### 流程图

```
┌────────────────────────────────────────────────────────────────┐
│                      HybridRetrieval                            │
│                                                                 │
│  hybrid_search(query, top_k)                                    │
│         │                                                       │
│         ├──> 并行执行两种检索                                   │
│         │                                                       │
│         ├──> BM25检索 (关键词匹配)                             │
│         │     │                                                 │
│         │     └──> bm25_search(query, top_k*2)                 │
│         │           │                                           │
│         │           ├──> 中文分词（jieba.lcut）                │
│         │           │     query → [token1, token2, ...]        │
│         │           │                                           │
│         │           ├──> BM25评分                               │
│         │           │     └─> get_scores(query_tokens)         │
│         │           │          └─> 计算每个文档的BM25分数      │
│         │           │                                           │
│         │           └──> 返回top_k*2个高分文档                 │
│         │                 (score > 0)                           │
│         │                                                       │
│         └──> 向量检索 (语义匹配)                                │
│               │                                                 │
│               └──> vector_search(query, top_k*2)               │
│                     │                                           │
│                     ├──> 获取query的embedding                  │
│                     │                                           │
│                     ├──> ChromaDB向量相似度搜索                │
│                     │                                           │
│                     └──> 返回top_k*2个相似文档                 │
│                                                                 │
│         ┌────────────────────────────────────────┐             │
│         │  Reciprocal Rank Fusion (RRF)          │             │
│         │                                         │             │
│         │  RRF分数计算：                          │             │
│         │  score(doc) = Σ 1/(k + rank + 1)       │             │
│         │                                         │             │
│         │  对于每个文档：                          │             │
│         │  - 如果在BM25结果中：                   │             │
│         │    score += 1/(60 + bm25_rank + 1)     │             │
│         │  - 如果在向量结果中：                   │             │
│         │    score += 1/(60 + vector_rank + 1)   │             │
│         └────────────────────────────────────────┘             │
│                          │                                      │
│                          └──> 按RRF分数排序                    │
│                                └─> 返回top_k个文档              │
│                                                                 │
│  返回：融合后的top_k个最相关文档                                │
└────────────────────────────────────────────────────────────────┘
```

### BM25检索原理

```
文档d对查询q的BM25分数：

BM25(d,q) = Σ IDF(qi) · f(qi,d)·(k1+1)
            qi∈q      ─────────────────────────
                      f(qi,d) + k1·(1-b+b·|d|/avgdl)

其中：
- IDF(qi): 逆文档频率（越少文档包含该词，权重越高）
- f(qi,d): 词qi在文档d中的频率
- |d|: 文档d的长度
- avgdl: 所有文档的平均长度
- k1, b: 调节参数（默认k1=1.5, b=0.75）
```

### RRF融合算法

```
假设：
BM25检索结果：[doc1, doc3, doc5] (按分数排序)
向量检索结果：[doc2, doc3, doc1] (按相似度排序)

RRF分数计算（k=60）：

doc1:
  BM25 rank=0 → score += 1/(60+0+1) = 0.0164
  向量 rank=2 → score += 1/(60+2+1) = 0.0159
  total = 0.0323

doc2:
  向量 rank=0 → score += 1/(60+0+1) = 0.0164
  total = 0.0164

doc3:
  BM25 rank=1 → score += 1/(60+1+1) = 0.0161
  向量 rank=1 → score += 1/(60+1+1) = 0.0161
  total = 0.0322

doc5:
  BM25 rank=2 → score += 1/(60+2+1) = 0.0159
  total = 0.0159

最终排序：doc1 > doc3 > doc2 > doc5
```

### 关键函数说明

#### `HybridRetrieval.build_bm25_index(documents)`
```python
功能：构建BM25索引
参数：documents - 所有文档列表
返回：无
处理：
  1. 保存文档列表
  2. 对每个文档进行中文分词（jieba）
  3. 使用分词结果构建BM25索引
```

#### `HybridRetrieval.hybrid_search(query, top_k)`
```python
功能：混合检索主函数
参数：
  - query: 查询文本
  - top_k: 返回结果数量
返回：融合后的文档列表
处理：
  1. 分别执行BM25和向量检索（各取top_k*2）
  2. 使用RRF算法融合结果
  3. 返回融合后的top_k个文档
```

---

## 5. 习题生成流程

### 流程图

```
┌────────────────────────────────────────────────────────────────┐
│                     generate_quiz()                             │
│                                                                 │
│  输入：topic（主题，可选）, difficulty（难度）                  │
│         │                                                       │
│         ├──> 获取课程内容                                       │
│         │     │                                                 │
│         │     ├──> if topic为空:                               │
│         │     │     └─> get_weighted_random_content(agent)     │
│         │     │          │                                      │
│         │     │          ├──> 获取所有文档                      │
│         │     │          │     └─> vector_store.get_all_documents() │
│         │     │          │                                      │
│         │     │          ├──> 计算权重                          │
│         │     │          │     ├─> 作业/考试相关: weight=2.5   │
│         │     │          │     ├─> 解答相关: weight=2.0        │
│         │     │          │     ├─> 讲义/课程: weight=1.8       │
│         │     │          │     ├─> 复习/总结: weight=1.5       │
│         │     │          │     └─> 其他: weight=1.0            │
│         │     │          │                                      │
│         │     │          └──> 加权随机选择一个文档              │
│         │     │               └─> random.choices(docs, weights) │
│         │     │                                                 │
│         │     └──> else (指定topic):                           │
│         │           └─> retrieve_context(topic, top_k=2)       │
│         │                └─> 检索相关内容                       │
│         │                                                       │
│         ├──> 构建出题Prompt                                     │
│         │     └─> "根据以下课程内容，生成一道{difficulty}难度   │
│         │          的单选题。                                   │
│         │          课程内容：{context}                          │
│         │          请以JSON格式返回：                           │
│         │          {                                            │
│         │            'question': '题目内容',                    │
│         │            'options': ['A...', 'B...', 'C...', 'D...'], │
│         │            'correct_answer': 'A',                    │
│         │            'explanation': '答案解析'                  │
│         │          }"                                           │
│         │                                                       │
│         ├──> LLM生成习题                                        │
│         │     └─> client.chat.completions.create()             │
│         │          ├─> System: "你是专业的出题专家"             │
│         │          └─> User: [上述Prompt]                      │
│         │                                                       │
│         └──> 返回JSON格式的习题                                 │
│                                                                 │
│  返回：JSON字符串（包含题目、选项、答案、解析）                  │
└────────────────────────────────────────────────────────────────┘
```

### 加权随机选择策略

```
目的：从知识库中智能选择适合出题的内容

权重设计：
┌──────────────────────────────────┬──────────┐
│ 文件名关键词                      │ 权重     │
├──────────────────────────────────┼──────────┤
│ homework, hw, 作业, exam, 考试, test │ 2.5  │  ← 最高优先级（考试和作业材料）
│ solution, sol, 解答, answer, 答案    │ 2.0  │  ← 高优先级（包含答案的材料）
│ lecture, 讲义, course, 课程         │ 1.8  │  ← 中高优先级（核心课程内容）
│ review, 复习, summary, 梳理         │ 1.5  │  ← 中等优先级（总结性材料）
│ 其他                                │ 1.0  │  ← 基础权重
└──────────────────────────────────┴──────────┘

选择算法：
1. 遍历所有文档，根据文件名计算权重
2. 使用random.choices()进行加权随机选择
3. 返回选中文档的内容
```

### 关键函数说明

#### `generate_quiz(agent, topic, difficulty)`
```python
功能：自动生成习题
参数：
  - agent: RAGAgent实例
  - topic: 主题（空字符串表示随机）
  - difficulty: 难度（简单/中等/困难）
返回：JSON格式的习题字符串
流程：
  1. 获取课程内容（随机或指定主题）
  2. 构建出题prompt
  3. 调用LLM生成习题
  4. 返回JSON格式结果
```

#### `get_weighted_random_content(agent)`
```python
功能：加权随机选择课程内容
参数：agent - RAGAgent实例
返回：选中的文档内容
流程：
  1. 获取所有文档
  2. 根据文件名计算权重
  3. 加权随机选择
  4. 返回文档内容
```

---

## 6. 完整系统架构

### 系统层次结构

```
┌─────────────────────────────────────────────────────────────────┐
│                        用户界面层                                 │
│  ┌─────────────────────┐    ┌──────────────────────────────┐   │
│  │   Streamlit Web界面  │    │   命令行界面 (main.py)        │   │
│  │   (uis/app.py)      │    │   - 交互式对话                 │   │
│  │   - 聊天界面         │    │   - 简单直接                   │   │
│  │   - 知识库管理       │    └──────────────────────────────┘   │
│  │   - 习题生成         │                                        │
│  └─────────────────────┘                                        │
└───────────────────────────┬─────────────────────────────────────┘
                            │
                            ↓
┌─────────────────────────────────────────────────────────────────┐
│                        应用逻辑层                                 │
│  ┌────────────────────────────────────────────────────────┐    │
│  │                    RAGAgent (rag_agent.py)              │    │
│  │  ┌──────────────────────────────────────────────────┐  │    │
│  │  │  answer_question()  - 完整问答流程                │  │    │
│  │  │    ├─> retrieve_context()  - 检索上下文          │  │    │
│  │  │    └─> generate_response() - 生成回答            │  │    │
│  │  └──────────────────────────────────────────────────┘  │    │
│  └────────────────────────────────────────────────────────┘    │
└───────────────────┬────────────────────┬────────────────────────┘
                    │                    │
                    ↓                    ↓
┌──────────────────────────┐  ┌──────────────────────────────────┐
│      检索层               │  │        生成层                     │
│  ┌──────────────────┐    │  │  ┌────────────────────────────┐ │
│  │  VectorStore     │    │  │  │    OpenAI LLM API          │ │
│  │  - 向量检索       │    │  │  │    (Qwen3-max)             │ │
│  └──────────────────┘    │  │  │    - 问答生成               │ │
│          ↕               │  │  │    - 习题生成               │ │
│  ┌──────────────────┐    │  │  │    - 多轮对话               │ │
│  │ HybridRetrieval  │    │  │  └────────────────────────────┘ │
│  │  - BM25检索      │    │  │                                  │
│  │  - 向量检索       │    │  └──────────────────────────────────┘
│  │  - RRF融合       │    │
│  └──────────────────┘    │
└───────────────────────────┘
                    ↑
                    │
┌──────────────────────────────────────────────────────────────────┐
│                        数据处理层                                  │
│  ┌────────────────┐  ┌──────────────┐  ┌───────────────────┐   │
│  │ DocumentLoader │→ │ TextSplitter │→ │ VectorStore       │   │
│  │ - 加载文档      │  │ - 切分文本    │  │ - 向量化          │   │
│  │ - 多格式支持    │  │ - 保持上下文  │  │ - 存储到ChromaDB  │   │
│  └────────────────┘  └──────────────┘  └───────────────────┘   │
└──────────────────────────────────────────────────────────────────┘
                    ↑
                    │
┌──────────────────────────────────────────────────────────────────┐
│                        数据存储层                                  │
│  ┌──────────────┐  ┌────────────────────────────────────────┐   │
│  │  data/       │  │         vector_db/                      │   │
│  │  (原始文档)   │  │         (ChromaDB)                      │   │
│  │  - PDF       │  │  - 文档向量                              │   │
│  │  - PPTX      │  │  - 元数据                                │   │
│  │  - DOCX      │  │  - 索引                                  │   │
│  │  - TXT       │  │                                         │   │
│  │  - 图片      │  │                                         │   │
│  └──────────────┘  └────────────────────────────────────────┘   │
└──────────────────────────────────────────────────────────────────┘
```

### 主要数据流

#### 1. 知识库构建流程

```
数据准备：将课程文档放入 data/ 目录
    ↓
执行：python process_data.py
    ↓
┌─────────────────────────────────────────────┐
│ 1. DocumentLoader.load_all_documents()      │
│    → 加载所有文档                            │
└─────────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────────┐
│ 2. TextSplitter.split_documents()           │
│    → 切分长文档                              │
└─────────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────────┐
│ 3. VectorStore.add_documents()              │
│    → 生成向量并存储                          │
└─────────────────────────────────────────────┘
    ↓
知识库构建完成，存储在 vector_db/ 目录
```

#### 2. 问答流程

```
用户提问
    ↓
┌─────────────────────────────────────────────┐
│ RAGAgent.answer_question(query)             │
└─────────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────────┐
│ 1. retrieve_context()                       │
│    ├─ 向量检索 or 混合检索                   │
│    └─ 返回相关文档                           │
└─────────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────────┐
│ 2. generate_response()                      │
│    ├─ 构建prompt（含检索内容）               │
│    └─ 调用LLM生成回答                        │
└─────────────────────────────────────────────┘
    ↓
返回答案给用户
```

#### 3. 习题生成流程

```
用户点击"生成习题"
    ↓
┌─────────────────────────────────────────────┐
│ 1. get_weighted_random_content()            │
│    → 加权随机选择课程内容                    │
└─────────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────────┐
│ 2. generate_quiz()                          │
│    ├─ 构建出题prompt                         │
│    └─ 调用LLM生成JSON格式习题                │
└─────────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────────┐
│ 3. 解析并展示习题                            │
│    ├─ 题目                                   │
│    ├─ 选项                                   │
│    ├─ 正确答案                               │
│    └─ 解析                                   │
└─────────────────────────────────────────────┘
```

### 核心技术栈

```
┌────────────────────────────────────────────────┐
│ 前端界面                                        │
│  - Streamlit (Web UI)                          │
└────────────────────────────────────────────────┘

┌────────────────────────────────────────────────┐
│ 后端框架                                        │
│  - Python 3.x                                  │
│  - OpenAI Python SDK (API调用)                 │
└────────────────────────────────────────────────┘

┌────────────────────────────────────────────────┐
│ 文档处理                                        │
│  - PyPDF2 (PDF解析)                            │
│  - python-pptx (PPTX解析)                      │
│  - docx2txt (DOCX解析)                         │
│  - EasyOCR (图片文字识别)                       │
└────────────────────────────────────────────────┘

┌────────────────────────────────────────────────┐
│ 检索技术                                        │
│  - ChromaDB (向量数据库)                        │
│  - OpenAI Embeddings (文本向量化)              │
│  - BM25 (传统检索算法)                          │
│  - jieba (中文分词)                             │
│  - RRF (结果融合算法)                           │
└────────────────────────────────────────────────┘

┌────────────────────────────────────────────────┐
│ 大语言模型                                      │
│  - Qwen3-max (阿里云通义千问)                   │
│  - OpenAI兼容接口                               │
└────────────────────────────────────────────────┘
```

### 配置参数说明

```python
# config.py 中的关键参数

# API配置
OPENAI_API_KEY = ""              # API密钥
OPENAI_API_BASE = "..."          # API基础URL
MODEL_NAME = "qwen3-max"         # LLM模型
OPENAI_EMBEDDING_MODEL = "..."   # Embedding模型

# 数据配置
DATA_DIR = "./data"              # 原始文档目录
VECTOR_DB_PATH = "./vector_db"   # 向量库路径
COLLECTION_NAME = "..."          # 集合名称

# 文本处理
CHUNK_SIZE = 1000                # 文档块大小（字符）
CHUNK_OVERLAP = 100              # 块之间重叠（字符）
MAX_TOKENS = 1500                # 最大token数

# RAG配置
TOP_K = 5                        # 检索文档数量
```

---

## 使用示例

### 命令行模式

```bash
# 1. 构建知识库
python process_data.py

# 2. 启动对话
python main.py

# 交互示例：
学生: 什么是深度学习？
助教: [基于检索到的课程材料回答...]
```

### Web界面模式

```bash
# 启动Web界面
streamlit run uis/app.py

# 浏览器访问：http://localhost:8501
# - 点击"重建知识库"按钮
# - 在对话框中提问
# - 点击"生成习题"按钮自动出题
```

---

## 性能优化建议

### 1. 检索优化
- 调整TOP_K参数（平衡召回率和准确率）
- 启用混合检索（提高召回率）
- 调整chunk_size（影响检索粒度）

### 2. 生成优化
- 调整temperature参数（控制生成随机性）
- 优化prompt设计（提高回答质量）
- 使用chat_history（保持对话连贯性）

### 3. 系统优化
- 使用@st.cache_resource缓存Agent实例
- 批量处理文档（减少API调用）
- 使用异步处理（提高响应速度）

---

## 扩展方向

### 1. 功能扩展
- 添加多模态支持（图片问答）
- 添加引用追溯功能
- 添加知识图谱构建
- 添加个性化推荐

### 2. 检索增强
- 添加重排序模型（Reranker）
- 支持多跳推理
- 添加查询改写
- 支持时间过滤

### 3. 界面增强
- 添加历史记录管理
- 添加知识库可视化
- 添加用户反馈收集
- 添加统计分析面板
